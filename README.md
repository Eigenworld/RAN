#  Random Attention Neural Netwoks


### <p align="center"> We aim to better evaluate the expressive power of graph neural networks (GNNs) and propose a simple yet effective model evaluation architecture, the random attention networks (RANs), based solely on a random aggregation process, dispensing with activation functions and feature transformations. The attentions in each layer are randomly generated and processed by the L2-normalized softmax without any learning. All the nodes follow the same but random rule to propagate their information. We can capture the true group structure from these varying-convergence trajectories of multi-head attentions. Experiments on benchmark networks show that our models outperform previous random aggregators, and achieve the accuracy comparable to the state-of-the-art graph neural networks. In this sense, these best known models may not be powerful as expected. This in turn shrinks the room for designing more innovative propagation layers. Moreover, Our model provides some interesting insights on how attentions influence the community detection and sheds light on the mechanism of message passing in graph neural networks.</p>
