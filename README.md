#  Random Attention Neural Netwoks


### <p align="left"> We aim to better evaluate the expressive power of graph neural networks (GNNs) and propose a simple yet effective model evaluation architecture, the random attention networks (RANs), based solely on a random aggregation process, dispensing with activation functions and feature transformations. The attentions in each layer are randomly generated and pro- cessed by a L2-normalized softmax without any learning. All the nodes follow the same but random rule to propagate their information. We can capture the true group structure from these varying-convergence trajectories of multi-head attentions. Experiments on benchmark networks show that our models outperform previous random aggregator methods and achieve the accuracy comparable to traditional state-of-the-art graph neural networks. We further develop a random attention based mixture model and employ a self-training method to train it on both labeled and pseudo labeled nodes. Results show that the mixture model compares favorably to current complex graph neural networks and almost achieves the best results for citation benchmark networks. In this sense, these best known and well- designed models may not be powerful as expected. This in turn shrinks the room for designing more innovative propagation layers. Moreover, the mechanism of message passing is fully explored in the context of graph drawing, and a unified geometric view of GNNs is first given.</p>
